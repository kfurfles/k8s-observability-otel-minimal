# =============================================================================
# MINIMAL OPENTELEMETRY STACK - ALL COMPONENTS IN ONE FILE
# Deploy: kubectl apply -f manifests/all-in-one.yaml
# =============================================================================

# -----------------------------------------------------------------------------
# NAMESPACE
# -----------------------------------------------------------------------------
apiVersion: v1
kind: Namespace
metadata:
  name: observability
  labels:
    name: observability

---
# -----------------------------------------------------------------------------
# OPENTELEMETRY COLLECTOR
# -----------------------------------------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: observability
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      batch:
        timeout: 5s
        send_batch_size: 512
      memory_limiter:
        limit_mib: 256
        check_interval: 1s
      # APM Enhancement: Resource processor for environment enrichment
      resource:
        attributes:
          - key: deployment.environment.name
            value: "development"
            action: upsert
          - key: service.namespace
            value: "default"
            action: upsert
          # Loki Correlation: Define which resource attributes become Loki labels
          - key: loki.resource.labels
            value: "service.name, service.version, deployment.environment.name, k8s.namespace.name, k8s.pod.name"
            action: insert
      # APM Enhancement: Attributes processor for service instance enrichment
      attributes:
        actions:
          - key: service.instance.id
            action: upsert
            from_attribute: service.name
          # Loki Correlation: Define which log attributes become Loki labels  
          - key: loki.attribute.labels
            value: "level, endpoint, method, status_code, error_type"
            action: insert

    exporters:
      # Updated: Send traces to Tempo instead of Jaeger
      otlp/tempo:
        endpoint: tempo:4317
        tls:
          insecure: true
        timeout: 30s
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
      prometheusremotewrite:
        endpoint: http://prometheus:9090/api/v1/write
        tls:
          insecure: true
        timeout: 30s
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
        # APM Enhancement: Enable resource-to-telemetry conversion for better APM labeling
        resource_to_telemetry_conversion:
          enabled: true
      # Log Correlation: Send logs to Loki with trace correlation
      loki:
        endpoint: http://loki:3100/loki/api/v1/push
        tls:
          insecure: true
        timeout: 30s
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
      logging:
        loglevel: info

    extensions:
      health_check:
        endpoint: 0.0.0.0:13133

    service:
      extensions: [health_check]
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, resource, attributes, batch]
          exporters: [otlp/tempo, logging]  # Changed from otlp/jaeger to otlp/tempo
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, resource, attributes, batch]
          exporters: [prometheusremotewrite, logging]
        # Log Correlation: Pipeline for structured logs with trace correlation
        logs:
          receivers: [otlp]
          processors: [memory_limiter, resource, attributes, batch]
          exporters: [loki, logging]
      telemetry:
        metrics:
          address: 0.0.0.0:8888

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
spec:
  replicas: 2
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: otel-collector
              topologyKey: kubernetes.io/hostname
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.88.0
        args:
          - "--config=/etc/config/config.yaml"
        ports:
        - name: otlp-grpc
          containerPort: 4317
        - name: otlp-http
          containerPort: 4318
        - name: metrics
          containerPort: 8888
        - name: health-check
          containerPort: 13133
        volumeMounts:
        - name: config-volume
          mountPath: /etc/config
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 1
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: config-volume
        configMap:
          name: otel-collector-config

---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
spec:
  selector:
    app: otel-collector
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
  - name: otlp-http
    port: 4318
    targetPort: 4318
  - name: metrics
    port: 8888
    targetPort: 8888

---
# HorizontalPodAutoscaler for OpenTelemetry Collector
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: otel-collector-hpa
  namespace: observability
  labels:
    app: otel-collector
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: otel-collector
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

---
# -----------------------------------------------------------------------------
# PROMETHEUS
# -----------------------------------------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: observability
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
    
    scrape_configs:
      - job_name: 'prometheus'
        static_configs:
        - targets: ['localhost:9090']
      
      - job_name: 'otel-collector'
        static_configs:
        - targets: ['otel-collector:8888']
        scrape_interval: 10s

---

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prometheus
  namespace: observability
  labels:
    app: prometheus
spec:
  serviceName: prometheus-headless
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: prometheus
              topologyKey: kubernetes.io/hostname
      containers:
      - name: prometheus
        image: prom/prometheus:v2.45.0
        args:
          - '--config.file=/etc/prometheus/prometheus.yml'
          - '--storage.tsdb.path=/prometheus/'
          - '--web.console.libraries=/etc/prometheus/console_libraries'
          - '--web.console.templates=/etc/prometheus/consoles'
          - '--storage.tsdb.retention.time=15d'
          - '--web.enable-lifecycle'
          - '--web.enable-remote-write-receiver'
          - '--web.enable-admin-api'
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: prometheus-config-volume
          mountPath: /etc/prometheus/
        - name: prometheus-data
          mountPath: /prometheus/
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2
            memory: 4Gi
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9090
          initialDelaySeconds: 30
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9090
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: prometheus-config-volume
        configMap:
          name: prometheus-config
  volumeClaimTemplates:
  - metadata:
      name: prometheus-data
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 50Gi

---
# Headless service for StatefulSet
apiVersion: v1
kind: Service
metadata:
  name: prometheus-headless
  namespace: observability
  labels:
    app: prometheus
spec:
  clusterIP: None
  selector:
    app: prometheus
  ports:
  - name: web
    port: 9090
    targetPort: 9090

---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: observability
  labels:
    app: prometheus
spec:
  selector:
    app: prometheus
  ports:
  - name: web
    port: 9090
    targetPort: 9090

---
# -----------------------------------------------------------------------------
# TEMPO (REPLACES JAEGER)
# -----------------------------------------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: tempo-config
  namespace: observability
  labels:
    app: tempo
data:
  tempo.yaml: |
    server:
      http_listen_port: 3200
      grpc_listen_port: 9095

    distributor:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318

    ingester:
      max_block_duration: 5m

    storage:
      trace:
        backend: s3
        s3:
          bucket: tempo-traces
          endpoint: minio.observability.svc.cluster.local:9000
          access_key: minio
          secret_key: minio123
          insecure: true
        wal:
          path: /var/tempo/wal
        local:
          path: /var/tempo/blocks

    compactor:
      compaction:
        block_retention: 24h

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tempo
  namespace: observability
  labels:
    app: tempo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tempo
  template:
    metadata:
      labels:
        app: tempo
    spec:
      containers:
      - name: tempo
        image: grafana/tempo:2.3.0
        args:
          - "-config.file=/etc/tempo/tempo.yaml"
        ports:
        - name: otlp-grpc
          containerPort: 4317
        - name: otlp-http
          containerPort: 4318
        - name: http
          containerPort: 3200
        - name: grpc
          containerPort: 9095
        volumeMounts:
        - name: config
          mountPath: /etc/tempo
        - name: storage
          mountPath: /var/tempo
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2
            memory: 2Gi
        livenessProbe:
          httpGet:
            path: /ready
            port: 3200
          initialDelaySeconds: 30
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /ready
            port: 3200
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: config
        configMap:
          name: tempo-config
      - name: storage
        persistentVolumeClaim:
          claimName: tempo-ingester-storage

---
apiVersion: v1
kind: Service
metadata:
  name: tempo
  namespace: observability
  labels:
    app: tempo
spec:
  selector:
    app: tempo
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
  - name: otlp-http
    port: 4318
    targetPort: 4318
  - name: http
    port: 3200
    targetPort: 3200
  - name: grpc
    port: 9095
    targetPort: 9095

---
# =============================================================================
# LOKI - LOG AGGREGATION WITH TRACE CORRELATION
# =============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: observability
data:
  loki.yaml: |
    auth_enabled: false
    
    server:
      http_listen_port: 3100
      http_listen_address: 0.0.0.0
      log_level: info
    
    common:
      instance_addr: 127.0.0.1
      path_prefix: /loki
      storage:
        filesystem:
          chunks_directory: /loki/chunks
          rules_directory: /loki/rules
      replication_factor: 1
      ring:
        kvstore:
          store: inmemory
    
    schema_config:
      configs:
        - from: 2020-10-24
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h
    
    analytics:
      reporting_enabled: false

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: loki
  namespace: observability
  labels:
    app: loki
spec:
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      containers:
      - name: loki
        image: grafana/loki:2.9.2
        args:
          - "-config.file=/etc/loki/loki.yaml"
        ports:
        - name: http
          containerPort: 3100
        volumeMounts:
        - name: config
          mountPath: /etc/loki
        - name: storage
          mountPath: /loki
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 1
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /ready
            port: 3100
          initialDelaySeconds: 30
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /ready
            port: 3100
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: config
        configMap:
          name: loki-config
      - name: storage
        persistentVolumeClaim:
          claimName: loki-storage

---
apiVersion: v1
kind: Service
metadata:
  name: loki
  namespace: observability
  labels:
    app: loki
spec:
  selector:
    app: loki
  ports:
  - name: http
    port: 3100
    targetPort: 3100

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: loki-storage
  namespace: observability
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

---
# -----------------------------------------------------------------------------
# GRAFANA
# -----------------------------------------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-config
  namespace: observability
data:
  grafana.ini: |
    [server]
    http_port = 3000
    domain = localhost
    
    [security]
    admin_user = admin
    admin_password = admin
    
    [users]
    allow_sign_up = false

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: observability
data:
  datasources.yaml: |
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus:9090
      isDefault: true
      editable: true
      jsonData:
        exemplarTraceIdDestinations:
          - name: TraceID
            datasourceUid: tempo
            urlDisplayLabel: "View Trace"
        timeInterval: "15s"
        queryTimeout: "30s"
    - name: Tempo
      type: tempo
      access: proxy
      url: http://tempo:3200
      editable: true
      uid: tempo
      jsonData:
        serviceMap:
          datasourceUid: 'prometheus'
        nodeGraph:
          enabled: true
        search:
          hide: false
        traceQuery:
          timeShiftEnabled: true
          spanStartTimeShift: "1h"
          spanEndTimeShift: "1h"
        tracesToLogs:
          datasourceUid: 'loki'
          tags: ['service.name', 'service.version', 'k8s.namespace.name']
          mappedTags: [{ key: 'service.name', value: 'service_name' }]
          mapTagNamesEnabled: true
          spanStartTimeShift: '-30s'
          spanEndTimeShift: '30s'
          filterByTraceID: true
          filterBySpanID: false
    # Log Correlation: Loki datasource for trace-log correlation
    - name: Loki
      type: loki
      access: proxy
      url: http://loki:3100
      editable: true
      uid: loki
      jsonData:
        derivedFields:
          - name: TraceID
            matcherRegex: 'trace_id[=:]([0-9a-f]{16,32})'
            url: '$${__value.raw}'
            datasourceUid: tempo
            urlDisplayLabel: 'View Trace'
          - name: SpanID
            matcherRegex: 'span_id[=:]([0-9a-f]{16})'
            url: '$${__value.raw}'
        maxLines: 5000

---
# Grafana Dashboards (Provisioning + APM Dashboard)
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: observability
data:
  dashboards.yaml: |
    apiVersion: 1
    providers:
    - name: 'default'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: false
      editable: true
      options:
        path: /etc/grafana/provisioning/dashboards
  placeholder.json: |
    {
        "id": null,
        "uid": "apm-routes-status",
        "title": "APM - Rotas e Status",
        "description": "Dashboard focado em rotas mais requisitadas e status codes",
        "tags": ["apm", "opentelemetry"],
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "🔥 Total de Requisições por Endpoint",
            "type": "stat",
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
            "targets": [
              {
                "expr": "sum by (endpoint) (http_requests_total)",
                "legendFormat": "{{endpoint}}",
                "refId": "A"
              }
            ],
            "options": {
              "orientation": "horizontal"
            },
            "fieldConfig": {
              "defaults": {
                "unit": "short",
                "color": {"mode": "palette-classic"}
              }
            }
          },
          {
            "id": 2,
            "title": "📊 Requests por Status Code",
            "type": "stat",
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
            "targets": [
              {
                "expr": "sum by (status_code) (http_requests_total)",
                "legendFormat": "HTTP {{status_code}}",
                "refId": "A"
              }
            ],
            "options": {
              "orientation": "horizontal"
            },
            "fieldConfig": {
              "defaults": {
                "unit": "short",
                "color": {"mode": "thresholds"},
                "thresholds": {
                  "steps": [
                    {"color": "green", "value": null},
                    {"color": "yellow", "value": 400},
                    {"color": "red", "value": 500}
                  ]
                }
              }
            }
          },
          {
            "id": 3,
            "title": "📈 Taxa de Requisições por Endpoint",
            "type": "timeseries",
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
            "targets": [
              {
                "expr": "sum by (endpoint) (rate(http_requests_total[5m]) * 60)",
                "legendFormat": "{{endpoint}}",
                "refId": "A"
              }
            ],
            "options": {
              "legend": {"displayMode": "list", "placement": "bottom"}
            },
            "fieldConfig": {
              "defaults": {
                "unit": "reqpm",
                "min": 0
              }
            }
          },
          {
            "id": 4,
            "title": "⚡ Latência P95 por Endpoint",
            "type": "timeseries",
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
            "targets": [
              {
                "expr": "histogram_quantile(0.95, sum by (endpoint, le) (rate(http_request_duration_seconds_bucket[5m])))",
                "legendFormat": "P95 {{endpoint}}",
                "refId": "A"
              }
            ],
            "options": {
              "legend": {"displayMode": "list", "placement": "bottom"}
            },
            "fieldConfig": {
              "defaults": {
                "unit": "s",
                "min": 0
              }
            }
          },
          {
            "id": 5,
            "title": "🚨 Apenas Erros (4xx/5xx)",
            "type": "timeseries",
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16},
            "targets": [
              {
                "expr": "sum by (status_code) (rate(http_requests_total{status_code=~\"4..|5..\"}[5m]) * 60)",
                "legendFormat": "HTTP {{status_code}}",
                "refId": "A"
              }
            ],
            "options": {
              "legend": {"displayMode": "list", "placement": "bottom"}
            },
            "fieldConfig": {
              "defaults": {
                "unit": "short",
                "min": 0
              }
            }
          },
          {
            "id": 6,
            "title": "📊 Métricas do Sistema",
            "type": "stat",
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 16},
            "targets": [
              {
                "expr": "sum(rate(otelcol_exporter_sent_spans[5m]) * 60)",
                "legendFormat": "Spans/min",
                "refId": "A"
              },
              {
                "expr": "sum(rate(http_requests_total[5m]) * 60)",
                "legendFormat": "HTTP Req/min",
                "refId": "B"
              }
            ],
            "options": {
              "orientation": "vertical"
            },
            "fieldConfig": {
              "defaults": {
                "unit": "short",
                "color": {"mode": "palette-classic"}
              }
            }
          }
        ],
        "time": {"from": "now-30m", "to": "now"},
        "refresh": "5s",
        "schemaVersion": 30,
        "version": 2
      }
    }

---
# PersistentVolumeClaim for Grafana shared storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana-storage
  namespace: observability
  labels:
    app: grafana
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: observability
  labels:
    app: grafana
spec:
  replicas: 2
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: grafana
              topologyKey: kubernetes.io/hostname
      containers:
      - name: grafana
        image: grafana/grafana:10.1.0
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_USER
          value: admin
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: admin
        - name: GF_PATHS_DATA
          value: /var/lib/grafana
        - name: GF_DATABASE_TYPE
          value: sqlite3
        - name: GF_DATABASE_PATH
          value: /var/lib/grafana/grafana.db
        volumeMounts:
        - name: grafana-config
          mountPath: /etc/grafana/grafana.ini
          subPath: grafana.ini
        - name: grafana-datasources
          mountPath: /etc/grafana/provisioning/datasources
        - name: grafana-dashboards
          mountPath: /etc/grafana/provisioning/dashboards
        - name: grafana-storage
          mountPath: /var/lib/grafana
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 1
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: grafana-config
        configMap:
          name: grafana-config
      - name: grafana-datasources
        configMap:
          name: grafana-datasources
      - name: grafana-dashboards
        configMap:
          name: grafana-dashboards
      - name: grafana-storage
        persistentVolumeClaim:
          claimName: grafana-storage

---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: observability
  labels:
    app: grafana
spec:
  selector:
    app: grafana
  ports:
  - name: web
    port: 3000
    targetPort: 3000

---
# -----------------------------------------------------------------------------
# STORAGE - Required for Tempo
# -----------------------------------------------------------------------------
# Tempo Ingester storage for active trace ingestion
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: tempo-ingester-storage
  namespace: observability
  labels:
    app: tempo-ingester
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi

---
# MinIO storage for long-term trace storage (object storage)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: minio-storage
  namespace: observability
  labels:
    app: minio
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi

---
# -----------------------------------------------------------------------------
# MINIO - Object storage for Tempo traces
# -----------------------------------------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: minio-config
  namespace: observability
  labels:
    app: minio
data:
  MINIO_ROOT_USER: "minio"
  MINIO_ROOT_PASSWORD: "minio123"
  MINIO_BROWSER: "on"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio
  namespace: observability
  labels:
    app: minio
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      labels:
        app: minio
    spec:
      containers:
      - name: minio
        image: minio/minio:latest
        args:
          - server
          - /data
          - --console-address
          - ":9001"
        env:
        - name: MINIO_ROOT_USER
          valueFrom:
            configMapKeyRef:
              name: minio-config
              key: MINIO_ROOT_USER
        - name: MINIO_ROOT_PASSWORD
          valueFrom:
            configMapKeyRef:
              name: minio-config
              key: MINIO_ROOT_PASSWORD
        ports:
        - name: api
          containerPort: 9000
        - name: console
          containerPort: 9001
        volumeMounts:
        - name: storage
          mountPath: /data
        resources:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            cpu: 1
            memory: 2Gi
        livenessProbe:
          httpGet:
            path: /minio/health/live
            port: 9000
          initialDelaySeconds: 30
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /minio/health/ready
            port: 9000
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: minio-storage

---
apiVersion: v1
kind: Service
metadata:
  name: minio
  namespace: observability
  labels:
    app: minio
spec:
  selector:
    app: minio
  ports:
  - name: api
    port: 9000
    targetPort: 9000
  - name: console
    port: 9001
    targetPort: 9001

---
# MinIO bucket initialization job
apiVersion: batch/v1
kind: Job
metadata:
  name: minio-bucket-init
  namespace: observability
  labels:
    app: minio-init
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: mc
        image: minio/mc:latest
        env:
        - name: MINIO_ROOT_USER
          valueFrom:
            configMapKeyRef:
              name: minio-config
              key: MINIO_ROOT_USER
        - name: MINIO_ROOT_PASSWORD
          valueFrom:
            configMapKeyRef:
              name: minio-config
              key: MINIO_ROOT_PASSWORD
        command:
        - /bin/sh
        - -c
        - |
          # Wait for MinIO to be ready
          until mc alias set minio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}; do
            echo "Waiting for MinIO to be ready..."
            sleep 5
          done
          
          # Create bucket for Tempo traces
          mc mb minio/tempo-traces --ignore-existing
          echo "✅ Tempo traces bucket created"
          
          echo "🎉 MinIO initialization complete!"
