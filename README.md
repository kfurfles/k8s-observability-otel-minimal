# ðŸŽ¯ Production-Ready Minimal OpenTelemetry Stack

A **production-ready** yet **minimal** OpenTelemetry observability stack for Kubernetes featuring auto-scaling, high availability, and persistent storage.

> ðŸŽ“ **Learning OpenTelemetry?** Check out [DEVELOPMENT.md](DEVELOPMENT.md) for educational guide with local development workflow.

## ðŸš€ **What You Get**

- **Auto-scaling ingestion**: 5,000-25,000 spans/second capacity
- **High availability**: Grafana multi-replica deployment with shared storage  
- **Persistent storage**: Data survives restarts and redeployments
- **One-command deployment**: Complete stack in < 2 minutes
- **Production features**: HPA, StatefulSets, anti-affinity, resource limits

## ðŸ—ï¸ **Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        APPLICATIONS                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ OTLP (4317/4318)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               OPENTELEMETRY COLLECTOR                           â”‚
â”‚                 (HPA: 2-5 replicas)                            â”‚
â”‚                Load Balanced + Auto-scaling                     â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                 â”‚                     â”‚
      â–¼                 â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROMETHEUS  â”‚ â”‚   GRAFANA LOKI  â”‚ â”‚   GRAFANA TEMPO     â”‚
â”‚(StatefulSet)â”‚ â”‚ (Log Aggreg.)   â”‚ â”‚   (Tracing)         â”‚
â”‚+ Persist.Volâ”‚ â”‚ + Persistent    â”‚ â”‚ + MinIO Storage     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                 â”‚                     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      GRAFANA        â”‚
              â”‚    (2 replicas)     â”‚
              â”‚  + Shared Storage   â”‚
              â”‚ Traces + Metrics +  â”‚
              â”‚        Logs         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“¦ **Components**

| Component | Purpose | Replicas | Storage | Features |
|-----------|---------|----------|---------|----------|
| **OpenTelemetry Collector** | Trace/metric ingestion | 2-5 (HPA) | - | Auto-scaling, load balancing |
| **Grafana Tempo** | Distributed tracing backend | 1 | 20Gi + MinIO | Block storage, S3 compatibility |
| **Prometheus** | Metrics storage & querying | 1 | 50Gi | Metrics storage, 15-day retention |
| **Grafana** | Unified observability dashboard | 2 | 10Gi shared | Multi-replica, shared persistence |
| **Loki** | Log aggregation with trace correlation | 1 | 10Gi | Structured logs, trace correlation |
| **MinIO** | S3-compatible object storage | 1 | 100Gi | Trace block storage, backup ready |

### **Data Flow**
1. **Applications** â†’ Send traces/metrics/logs via OTLP
2. **OTel Collector** â†’ Routes traces to Tempo, metrics to Prometheus, logs to Loki  
3. **Tempo** â†’ Stores traces in MinIO with local WAL
4. **Prometheus** â†’ Stores metrics with persistent storage
5. **Loki** â†’ Stores structured logs with trace correlation
6. **Grafana** â†’ Unified querying of traces (Tempo), metrics (Prometheus), and logs (Loki)

## âš¡ **Production Features**

### **Auto-Scaling**
- **OpenTelemetry Collector**: 2-5 replicas based on CPU (70%) and memory (80%)
- **Scaling time**: 1-2 minutes response to load changes
- **Capacity**: Handles 5K-25K spans/second automatically

### **High Availability**
- **Prometheus**: Single StatefulSet with persistent storage
- **Grafana**: 2-replica deployment with shared storage  
- **Anti-affinity**: Pods distributed across nodes when possible
- **Uptime**: 99%+ availability target

### **Persistent Storage**
- **Total allocation**: 190Gi across 5 persistent volumes
- **Backup ready**: MinIO data can be backed up/restored
- **Retention**: 15 days metrics, 24 hours trace blocks (configurable)

## ðŸš€ **Quick Start**

### **Prerequisites**
- **Kubernetes cluster** (minikube, k3s, EKS, GKE, AKS)
- **kubectl** configured and connected
- **Python 3.7+** (for test application)
- **4+ CPU cores, 8Gi+ RAM** (for optimal performance)

### **Storage Requirements**
The stack uses cluster's **default storage class** automatically (180Gi total).

**Custom Storage Class (Optional):**
```bash
# Edit platform/manifests/all-in-one.yaml to add:
storageClassName: your-storage-class
```

**Common storage classes:**
- **minikube/kind**: `standard`
- **k3s**: `local-path`
- **EKS**: `gp2`, `gp3`
- **GKE**: `standard-rwo`
- **AKS**: `default`, `managed-premium`

### **Deploy (1 command)**
```bash
./scripts/up.sh
```

**What happens:**
- Deploys 28 Kubernetes resources
- Waits for all pods to be ready
- Sets up auto-scaling (HPA) 
- Configures port-forwards
- Shows access URLs

**Expected time:** < 2 minutes

### **Deploy Example Applications**
```bash
# Deploy instrumented applications to Kubernetes
./scripts/deploy-examples.sh
```

**What this deploys:**
- **Instrumented App**: Flask application with comprehensive OpenTelemetry instrumentation
- **Load Generator**: Locust-based realistic traffic generator
- **Auto-scaling ready**: Applications configured for HPA testing

### **Test Stack Performance**
```bash
# Automatically detects context and tests accordingly
./scripts/test-load.sh
```

**Context Detection:**
- **Kubernetes**: If port-forward active, tests K8s deployment
- **Local development**: Auto-starts app if needed for testing

**What this validates:**
- **Trace ingestion**: 5K-25K spans/second capacity
- **Auto-scaling**: HPA response to load changes
- **Production features**: HA, persistence, monitoring

### **Access Applications**
- **Grafana**: http://localhost:3000 (admin/admin)
  - Dashboards, metrics, alerts
  - **ðŸŽ¯ APM Dashboard**: http://localhost:3000/d/apm-routes-status
  - **Traces**: Explore â†’ Tempo
- **Trace ingestion**: 
  - HTTP: localhost:4318
  - gRPC: localhost:4317

### **Stop Everything**
```bash
./scripts/down.sh
```

## ðŸ“Š **Monitoring & Observability**

### **Check System Health**
```bash
# Pod status
kubectl get pods -n observability

# Auto-scaling status  
kubectl get hpa -n observability

# Storage usage
kubectl get pvc -n observability

# Resource usage
kubectl top pods -n observability
```

### **Access Logs**
```bash
# OpenTelemetry Collector
kubectl logs -n observability -l app=otel-collector

# Tempo
kubectl logs -n observability -l app=tempo

# Prometheus  
kubectl logs -n observability -l app=prometheus
```

## ðŸ”§ **Configuration**

### **Scaling Adjustment**
Edit HPA settings:
```bash
kubectl edit hpa otel-collector-hpa -n observability
```

### **Storage Expansion**  
Modify PVC sizes in `manifests/all-in-one.yaml`:
```yaml
# Example: Increase Prometheus storage
resources:
  requests:
    storage: 100Gi  # Change from 50Gi
```

### **Retention Policies**
- **Prometheus**: Edit `--storage.tsdb.retention.time` in manifest
- **Tempo**: Edit `block_retention` in Tempo config

## ðŸ” **Troubleshooting**

### **Common Issues**

| Issue | Cause | Solution |
|-------|-------|----------|
| Pods pending | Insufficient resources | Check `kubectl describe pod` |
| Port-forward fails | Port already in use | `pkill -f "kubectl port-forward"` |
| PVCs pending | No default storage class | Check `kubectl get storageclass` and configure if needed |
| High memory usage | Normal for observability stack | Monitor with `kubectl top pods` |

### **Resource Requirements**

| Environment | CPU | Memory | Storage |
|-------------|-----|--------|---------|
| **Minimal** | 2 cores | 5Gi | 180Gi |
| **Recommended** | 6 cores | 8Gi | 180Gi |
| **Production** | 12+ cores | 16Gi+ | 500Gi+ |

### **Performance Tuning**
```bash
# Check current resource usage
kubectl top pods -n observability

# Scale up if needed
kubectl scale deployment otel-collector -n observability --replicas=3

# Monitor HPA decisions
kubectl describe hpa otel-collector-hpa -n observability
```

## ðŸ“ˆ **Capacity Planning**

### **Expected Performance**

| Metric | Value |
|--------|-------|
| **Trace ingestion** | 5,000-25,000 spans/second |
| **Metric ingestion** | 10,000+ samples/second |
| **Query response** | < 3 seconds |
| **Storage growth** | ~1GB/day per 1K spans/second |

### **Scaling Indicators**
- **Scale up**: CPU > 70%, Memory > 80%, Query latency > 5s
- **Scale down**: CPU < 30%, Memory < 50% for > 10 minutes

## ðŸ”’ **Production Considerations**

### **Security**
- Change default Grafana credentials
- Enable TLS for external access
- Configure network policies
- Set up RBAC for service accounts

### **Backup Strategy**
```bash
# Backup MinIO data (traces)
kubectl exec -n observability minio-xxx -- tar czf /tmp/traces.tar.gz /data

# Backup Grafana dashboards
kubectl exec -n observability grafana-xxx -- tar czf /tmp/grafana.tar.gz /var/lib/grafana
```

### **Monitoring the Monitoring**
- Set up alerts on Prometheus storage usage
- Monitor OTel Collector error rates  
- Track Tempo ingestion lag
- Monitor pod restart counts

---

## ðŸ“‹ **Summary**

**Total files**: 9 | **Total lines**: ~2,600 | **Deploy time**: < 2 minutes  
**Production features**: Auto-scaling, persistence, monitoring  
**Testing options**: Load testing | **Minimal complexity**: One command deploy

ðŸŽ¯ **Perfect for**: Development teams wanting production-grade observability without operational complexity.

ðŸš€ **Ready for**: Production workloads, high-scale ingestion, enterprise requirements.